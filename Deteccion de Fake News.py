# -*- coding: utf-8 -*-
"""Proyecto_DS.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1g0UHkemWqMaZbhD8r9wtSOq5z079v1ey

# **Detección de Fake News**

***Integrantes***

Camila Gaete — 19728987-2

Luis Maiben — 20989516-1

Antonia Medina — 20958560-k

***Introducción***

Las fake news o noticias falsas son informaciones engañosas creadas deliberadamente para desinformar o manipular a las personas, difundidas a través de medios de comunicación y redes sociales. Estas noticias, que pueden parecer reales pero contienen datos falsos o sacados de contexto, son problemáticas porque influyen en la opinión pública, generan divisiones sociales, desconfianza en los medios de comunicación, y afectan decisiones importantes como elecciones políticas. En un mundo donde la información circula rápidamente y es difícil verificar su veracidad, las fake news tienen un impacto negativo significativo en la sociedad. A nivel grupal, la motivación sobre la temática surge de la importancia que tendrá, en el mundo profesional del futuro, contar con certeza sobre la información proveniente de los medios de comunicación. El grupo autor considera crucial que esta información sea verídica y un apoyo fiable al momento de tomar decisiones.

En los últimos años, la ciencia de datos ha jugado un papel crucial en la detección de noticias falsas mediante el uso de herramientas de inteligencia artificial y aprendizaje automático. Proyectos como FANDANGO han aplicado técnicas avanzadas, como el procesamiento del lenguaje natural y el análisis de imágenes, para identificar y analizar la desinformación en medios y redes sociales.

Para el proyecto, la entrada del algoritmo será el contenido textual de una noticia en inglés, incluyendo el título y el cuerpo de la noticia como principales características. El conjunto de datos utilizado contiene noticias etiquetadas como verdaderas (1) o falsas (0). Posteriormente, se aplicará un modelo de aprendizaje supervisado para predecir la veracidad de las noticias.

***Conjunto de datos y características***

El conjunto de datos utilizado en este proyecto es el WELFake, compuesto por un total de 72,134 artículos de noticias, cada uno con dos características principales: el título y el contenido del artículo, que sirven como datos de entrada, siendo éstos los atributos del dataset, donde el título corresponde al encabezado  de la noticia, y el contenido de artículo, el desarrollo de la noticia. Además, el dataset incluye una etiqueta que clasifica cada noticia como verdadera (1) o falsa (0), con 35,028 noticias reales y 37,106 noticias falsas, lo que permite un balance adecuado entre ambas clases para el entrenamiento del modelo. La etiqueta corresponde al output del dataset, que tal y como se menciona anteriormente, se clasifica con 0 y 1, teniendo datos de índole discretos, por lo que el modelo corresponderá a una clasificación.

***Clasificación de los Atributos del Dataset***

**Título de la noticia: Nominal**

Los títulos son cadenas de texto sin un orden intrínseco ni valores cuantificables. Cada título es único y no tiene un significado numérico o jerárquico, lo que los agrupa como categorías distintas sin posibilidad de comparaciones cuantitativas.

**Contenido del artículo: Nominal**

El contenido del artículo es un texto descriptivo que carece de un orden cuantitativo o jerárquico. Aunque varía en longitud y detalle, sigue siendo una agrupación categórica, sin criterios objetivos para comparaciones numéricas.

**Label: Nominal**

El label contiene etiquetas binarias que clasifican las noticias como falsas (0) o verdaderas (1). Aunque estos valores son numéricos, su interpretación es simplemente una asignación de categorías distintas, sin jerarquía ni valor cuantitativo, dado que no hay necesidad de establecer que una categoría es "mejor" que la otra en términos de datos.

***Un ejemplo de los datos...***

**Título:** "About Time! Christian Group Sues Amazon and SPLC for Designation as Hate Group"

**Texto:** "All we can say on this one is it s about time someone sued the Southern Poverty Law Center!On Tuesday, D. James Kennedy Ministries (DJKM) filed a lawsuit against the Southern Poverty Law Center (SPLC)..."

**Etiqueta:** 1 (Verdadero)

***Entregables***

**Must accomplish**

Preprocesamiento de Datos Textuales.

Evaluación de Rendimiento del Modelo.

Análisis Exploratorio de Datos.

**Expect to accomplish**

Recall de un 85%.

Visualización de Resultados de Clasificación.

Análisis de Importancia de Características.

**Would like to accomplish**

Despliegue del Modelo.

Integración de Explicabilidad del Modelo.

Integración de Feedback del Usuario.

***Carga y Visualización Inicial del Conjunto de Datos***

Este extracto de código permite importar un archivo CSV desde el sistema local a Google Colab, cargarlo en un DataFrame de pandas y mostrar las primeras filas para una vista rápida de su contenido.
"""

# Importa la biblioteca para cargar archivos desde el sistema local en Google Colab
from google.colab import files

# Importa la biblioteca pandas para trabajar con datos en formato de tablas (DataFrames)
import pandas as pd  # análisis estadístico

# Carga un archivo desde el sistema local (permite seleccionar el archivo desde el explorador)
uploaded = files.upload()

# Lee el archivo CSV cargado llamado 'WELFake_Dataset.csv' y lo guarda en un DataFrame llamado 'dataset'
dataset = pd.read_csv('WELFake_Dataset.csv')

# Muestra las primeras 5 filas del DataFrame para una vista rápida del contenido
print(dataset.head())

"""Antes de entrenar el modelo, se llevarán a cabo varias etapas de preprocesamiento en el conjunto de datos. En primer lugar, se realizará la limpieza del texto, eliminando stopwords, puntuación y convirtiendo todo a minúsculas para reducir la variabilidad.

Además, se aplicarán técnicas de lematización para simplificar las palabras a su forma base, lo que mejorará la consistencia del texto procesado. Para representar las características textuales, se utilizará el método TF-IDF (Term Frequency-Inverse Document Frequency), que considera la frecuencia de términos en un documento en relación con su frecuencia en el conjunto de documentos. Esto transforma el texto en vectores numéricos que reflejan la importancia relativa de cada palabra en el contexto global, asignando menor peso a las palabras comunes y mayor peso a las menos frecuentes pero más informativas.

Posteriormente, el conjunto de datos se dividirá en dos subconjuntos: entrenamiento y prueba. Estos subconjuntos se generarán aleatoriamente para garantizar que el modelo pueda generalizar adecuadamente a datos no vistos, asegurando así un buen desempeño en escenarios reales.

***Preprocesamiento de Texto y Vectorización para Modelado***

Este fragmento de código importa las librerías necesarias, limpia y transforma los datos textuales, elimina stopwords, aplica lematización, y finalmente vectoriza el texto usando TF-IDF para preparar el conjunto de datos para su posterior utilización en un modelo. También divide el conjunto en entrenamiento y prueba, y guarda los datos procesados para uso posterior.

Al utilizar técnicas de vectorización de texto como TF-IDF, se optimiza la representación del contenido, permitiendo que los modelos aplicables (como RF y RL presentados más adelante) extraigan características significativas que enriquecen su capacidad de clasificación, contribuyendo a una evaluación más completa de la veracidad de las noticias.
"""

# Importación de librerías para operaciones matemáticas, procesamiento de lenguaje natural, y modelado
import numpy as np  # Operaciones matemáticas y matrices
import pandas as pd  # Manipulación de datos
import nltk  # Procesamiento de lenguaje natural
import re  # Expresiones regulares para limpieza de texto
from sklearn.feature_extraction.text import TfidfVectorizer  # Vectorización de texto con TF-IDF
from sklearn.model_selection import train_test_split  # División del dataset en entrenamiento y prueba
from nltk.corpus import stopwords  # Palabras vacías (stopwords) en inglés
from nltk.stem import WordNetLemmatizer  # Lematización de palabras
from nltk.tokenize import word_tokenize  # Tokenización
import joblib  # Biblioteca para guardar modelos y datos

# Descargar recursos necesarios de NLTK
nltk.download('stopwords')  # Descarga las stopwords
nltk.download('wordnet')  # Descarga el lematizador WordNet
nltk.download('punkt_tab')  # Descarga el tokenizador

# Detección de duplicados (convertir listas a cadenas)
for column in ['tokenized_title', 'tokenized_text']:
    if column in dataset.columns:
        dataset[column] = dataset[column].apply(lambda x: ' '.join(x) if isinstance(x, list) else x)

dataset.drop_duplicates(inplace=True)  # Eliminar duplicados en el conjunto de datos

# Función para limpiar el texto eliminando URLs, puntuación y convirtiendo a minúsculas
def clean_text(text):
    if pd.isna(text):  # Verificar si el valor es NaN
        return ""  # Devolver una cadena vacía si es NaN
    text = re.sub(r'http\S+', '', text)  # Eliminar URLs que pueden introducir ruido en los datos
    text = re.sub(r'[^\w\s]', '', text)  # Eliminar puntuación que no es relevante para el análisis
    text = text.lower()  # Convertir a minúsculas para unificar el texto y evitar duplicados
    return text

# Función para reemplazar números con un token
def replace_numbers(text):
    return re.sub(r'\d+', '<NUM>', text)  # Reemplazar números por un token para evitar que influencien el análisis

# Función para eliminar el token <NUM>
def remove_num_token(text):
    return text.replace('<NUM>', '')  # Eliminar el token <NUM>

# Función para tokenizar el texto
def tokenize_text(text):
    return word_tokenize(text)  # Dividir el texto en palabras individuales para análisis posterior

# Definir el conjunto de stopwords (palabras comunes que no aportan significado)
stop_words = set(stopwords.words('english'))

# Función para eliminar stopwords del texto
def remove_stopwords(text):
    words = text.split()  # Dividir el texto en palabras
    words = [word for word in words if word not in stop_words]  # Filtrar stopwords que no aportan significado
    return ' '.join(words)  # Reconstruir el texto sin stopwords para centrarse en palabras relevantes

# Función para lematizar el texto
lemmatizer = WordNetLemmatizer()

def lemmatize_text(text):
    words = text.split()  # Dividir el texto en palabras
    words = [lemmatizer.lemmatize(word) for word in words]  # Lematizar cada palabra para reducirla a su forma base
    return ' '.join(words)  # Reconstruir el texto lematizado para mantener la integridad del contenido

# Aplicar las funciones de procesamiento a las columnas de 'title' y 'text'
dataset['cleaned_title'] = dataset['title'].apply(clean_text)  # Limpiar títulos
dataset['cleaned_text'] = dataset['text'].apply(clean_text)  # Limpiar textos

# Aplicar el reemplazo de números
dataset['cleaned_title'] = dataset['cleaned_title'].apply(replace_numbers)  # Reemplazar números en títulos
dataset['cleaned_text'] = dataset['cleaned_text'].apply(replace_numbers)  # Reemplazar números en textos

# Eliminar el token <NUM>
dataset['cleaned_title'] = dataset['cleaned_title'].apply(remove_num_token)
dataset['cleaned_text'] = dataset['cleaned_text'].apply(remove_num_token)

# Aplicar la tokenización
dataset['tokenized_title'] = dataset['cleaned_title'].apply(tokenize_text)  # Tokenizar títulos
dataset['tokenized_text'] = dataset['cleaned_text'].apply(tokenize_text)  # Tokenizar textos

# Aplicar la función de eliminación de stopwords
dataset['cleaned_title'] = dataset['cleaned_title'].apply(remove_stopwords)  # Eliminar stopwords de títulos
dataset['cleaned_text'] = dataset['cleaned_text'].apply(remove_stopwords)  # Eliminar stopwords de textos

# Aplicar la función de lematización
dataset['cleaned_title'] = dataset['cleaned_title'].apply(lemmatize_text)  # Lematizar títulos
dataset['cleaned_text'] = dataset['cleaned_text'].apply(lemmatize_text)  # Lematizar textos

# Verificación de NaNs
dataset.dropna(subset=['cleaned_title', 'cleaned_text'], inplace=True)  # Eliminar filas con NaN en columnas relevantes

# Combinar el título y el texto limpio en una nueva columna para la vectorización
dataset['combined'] = dataset['cleaned_title'] + ' ' + dataset['cleaned_text']  # Crear una representación combinada para análisis

# Vectorizar el texto utilizando TF-IDF (Term Frequency-Inverse Document Frequency)
vectorizer = TfidfVectorizer(max_features=5000)  # Limitar a las 5000 palabras más importantes para reducir la dimensionalidad
X = vectorizer.fit_transform(dataset['combined'])  # Aplicar TF-IDF a la columna combinada

# Etiquetas (dependiente) de la columna 'label' para clasificar las noticias como verdaderas o falsas
y = dataset['label']

# Dividir los datos en conjuntos de entrenamiento (80%) y prueba (20%)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)  # Proporción estándar para evaluación del modelo

# Guardar los datos procesados y el vectorizador TF-IDF para uso posterior
joblib.dump(vectorizer, 'tfidf_vectorizer.pkl')  # Guardar el vectorizador para reutilizarlo
joblib.dump((X_train, X_test, y_train, y_test), 'preprocessed_data.pkl')  # Guardar los conjuntos de datos divididos para su uso en entrenamiento y evaluación

"""El código presentado permitió avanzar de manera significativa en el preprocesamiento de los datos textuales del conjunto WELFake. En primer lugar, se realizó una limpieza básica de los textos, eliminando signos de puntuación, caracteres especiales y normalizando el contenido a minúsculas para asegurar la uniformidad en los datos. Además, se eliminaron un conjunto de stop words, es decir, palabras comunes y poco informativas como artículos y preposiciones, que no aportan valor significativo al análisis del contenido.

Otro paso fundamental fue la lematización de los textos, mediante la cual las palabras se redujeron a su forma base o raíz. Esto permitió normalizar las distintas variaciones de una misma palabra, simplificando el análisis posterior y reduciendo la dimensionalidad de los datos.

***Análisis de Longitud de Títulos y Textos***

Este código visualiza la distribución de la longitud de títulos y textos en un conjunto de datos, tanto originales como limpiados. Los histogramas permiten evaluar la complejidad del contenido y el impacto del preprocesamiento en su concisión.
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Visualizar la longitud de los títulos y textos originales
dataset['title_length'] = dataset['title'].str.len()  # Longitud de títulos originales
dataset['text_length'] = dataset['text'].str.len()    # Longitud de textos originales

plt.figure(figsize=(12, 6))

# Longitud de los títulos
plt.subplot(1, 2, 1)
sns.histplot(dataset['title_length'], bins=30, kde=True)
plt.title('Distribución de Longitud de Títulos Originales')
plt.xlabel('Longitud del Título')
plt.ylabel('Frecuencia')
# Esto muestra la longitud de los títulos originales, lo que permite evaluar su complejidad y claridad antes de cualquier transformación.

# Longitud de los textos
plt.subplot(1, 2, 2)
sns.histplot(dataset['text_length'], bins=30, kde=True)
plt.title('Distribución de Longitud de Textos Originales')
plt.xlabel('Longitud del Texto')
plt.ylabel('Frecuencia')
# Esta visualización muestra la longitud de los textos originales, ayudando a entender la cantidad de información antes del preprocesamiento.

plt.tight_layout()
plt.show()

# Visualizar la longitud de los títulos y textos limpiados
dataset['cleaned_title_length'] = dataset['cleaned_title'].str.len()  # Longitud de títulos limpiados
dataset['cleaned_text_length'] = dataset['cleaned_text'].str.len()    # Longitud de textos limpiados

plt.figure(figsize=(12, 6))

# Longitud de los títulos limpiados
plt.subplot(1, 2, 1)
sns.histplot(dataset['cleaned_title_length'], bins=30, kde=True)
plt.title('Distribución de Longitud de Títulos Limpiados')
plt.xlabel('Longitud del Título Limpio')
plt.ylabel('Frecuencia')
# Esta visualización muestra la longitud de los títulos después de la limpieza, permitiendo ver cómo las transformaciones afectaron su concisión y relevancia.

# Longitud de los textos limpiados
plt.subplot(1, 2, 2)
sns.histplot(dataset['cleaned_text_length'], bins=30, kde=True)
plt.title('Distribución de Longitud de Textos Limpiados')
plt.xlabel('Longitud del Texto Limpio')
plt.ylabel('Frecuencia')
# Aquí se observa la longitud de los textos después de la limpieza, lo cual es crucial para analizar cómo el preprocesamiento ha modificado el contenido y su usabilidad para el modelado.

plt.tight_layout()
plt.show()

"""***Visualización de Palabras Clave en Noticias Verdaderas y Falsas***

Este código genera nubes de palabras para visualizar las palabras más frecuentes en noticias verdaderas y falsas. Utiliza la biblioteca WordCloud para crear representaciones gráficas, donde el tamaño de cada palabra indica su frecuencia en el texto. La nube de palabras permite identificar rápidamente los términos más relevantes y recurrentes en cada categoría de noticias, facilitando un análisis cualitativo de su contenido.
"""

from wordcloud import WordCloud
import matplotlib.pyplot as plt

# Función para generar la nube de palabras
def generate_wordcloud(data, title):
    text = ' '.join(data)
    text = text.replace('<NUM>', '')  # Eliminar el token <NUM>
    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)

    plt.figure(figsize=(10, 6))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.axis('off')
    plt.title(title)
    plt.show()

# Nube de palabras para noticias verdaderas
generate_wordcloud(dataset[dataset['label'] == 1]['cleaned_text'], 'Nube de Palabras - Noticias Verdaderas')

# Nube de palabras para noticias falsas
generate_wordcloud(dataset[dataset['label'] == 0]['cleaned_text'], 'Nube de Palabras - Noticias Falsas')

"""***Análisis de Frecuencia de Palabras en Textos Limpios***

Este fragmento de código combina todos los textos limpios, calcula la frecuencia de las palabras utilizando la clase Counter, y extrae las 10 palabras más comunes. Luego, visualiza estas palabras y sus frecuencias en un gráfico de barras, lo que permite identificar los términos más recurrentes en el conjunto de datos, facilitando así el entendimiento del contenido predominante en los textos analizados.
"""

from collections import Counter
import matplotlib.pyplot as plt

# Combinar todos los títulos y textos limpios
all_words = ' '.join(dataset['cleaned_text']).split()
word_freq = Counter(all_words)

# Obtener las 10 palabras más comunes
most_common_words = word_freq.most_common(10)
words, counts = zip(*most_common_words)

# Graficar
plt.figure(figsize=(10, 6))
plt.bar(words, counts, color='skyblue')
plt.title('Palabras Más Frecuentes en Textos Limpios')
plt.xlabel('Palabras')
plt.ylabel('Frecuencia')
plt.xticks(rotation=45)
plt.show()

"""***Análisis de Palabras Comunes en Noticias Falsas y Verdaderas***

Este código cuenta y compara las palabras más comunes en noticias clasificadas como falsas y verdaderas, utilizando la biblioteca Counter. Genera listas de las diez palabras más frecuentes en cada categoría, facilitando la identificación de patrones léxicos que pueden ayudar a distinguir entre desinformación y contenido veraz.
"""

import matplotlib.pyplot as plt
from collections import Counter

# Noticias Falsas
false_news = ' '.join(dataset[dataset['label'] == 0]['cleaned_text']).split()
false_word_freq = Counter(false_news)

# Noticias Verdaderas
true_news = ' '.join(dataset[dataset['label'] == 1]['cleaned_text']).split()
true_word_freq = Counter(true_news)

# Las 10 palabras más comunes en noticias falsas
false_common_words = false_word_freq.most_common(10)

# Las 10 palabras más comunes en noticias verdaderas
true_common_words = true_word_freq.most_common(10)

# Extraer palabras y frecuencias
false_words, false_counts = zip(*false_common_words)
true_words, true_counts = zip(*true_common_words)

# Crear el gráfico
plt.figure(figsize=(12, 6))

# Subgráfico para noticias falsas
plt.subplot(1, 2, 1)
plt.bar(false_words, false_counts, color='red')
plt.title('Palabras más comunes en Noticias Falsas')
plt.xlabel('Palabras')
plt.ylabel('Frecuencia')
plt.xticks(rotation=45)

# Subgráfico para noticias verdaderas
plt.subplot(1, 2, 2)
plt.bar(true_words, true_counts, color='green')
plt.title('Palabras más comunes en Noticias Verdaderas')
plt.xlabel('Palabras')
plt.ylabel('Frecuencia')
plt.xticks(rotation=45)

plt.tight_layout()
plt.show()

"""***Análisis de Distribución de Etiquetas y Longitud de Contenido***

Este código permite visualizar la distribución de etiquetas (noticias verdaderas vs. falsas) con un gráfico de barras. Además, compara la longitud del contenido entre noticias falsas y verdaderas mediante un boxplot, revelando posibles diferencias en tamaño textual según la veracidad.
"""

import matplotlib.pyplot as plt
import seaborn as sns

# Distribución de las etiquetas en un gráfico de barras
plt.figure(figsize=(8, 6))
sns.countplot(x='label', data=dataset)
plt.title('Distribución de las Etiquetas (Noticias Verdaderas vs Falsas)')
plt.xlabel('Etiqueta')
plt.ylabel('Cantidad')
plt.show()

# Longitud de los títulos y del contenido (tamaño del texto en caracteres)
dataset['title_length'] = dataset['title'].fillna('').apply(len)
dataset['text_length'] = dataset['text'].fillna('').apply(len)  # Manejo de NaN en 'text'

# Comparar la longitud del contenido para noticias falsas vs verdaderas
plt.figure(figsize=(8, 6))
sns.boxplot(x='label', y='text_length', data=dataset)
plt.title('Comparación de la Longitud del Contenido entre Noticias Falsas y Verdaderas')
plt.xlabel('Etiqueta (0 = Falsa, 1 = Verdadera)')
plt.ylabel('Longitud del Contenido (número de caracteres)')
plt.show()

# Comparar la longitud del titulo para noticias falsas vs verdaderas
plt.figure(figsize=(8, 6))
sns.boxplot(x='label', y='title_length', data=dataset)
plt.title('Comparación de la Longitud del Título entre Noticias Falsas y Verdaderas')
plt.xlabel('Etiqueta (0: Falsas, 1: Verdaderas)')
plt.ylabel('Longitud del Título')
plt.xticks(ticks=[0, 1], labels=['Falsas', 'Verdaderas'])
plt.show()

"""***Análisis de Diversidad Léxica en Noticias Falsas y Verdaderas***

Este código calcula y visualiza la diversidad léxica (número de palabras únicas) para noticias falsas y verdaderas. Utiliza gráficos de barras para comparar esta métrica entre ambas clases, mostrando la riqueza léxica promedio en cada categoría.
"""

# Análisis de diversidad léxica
dataset['unique_word_count'] = dataset['cleaned_text'].apply(lambda x: len(set(x.split())))
diversity = dataset.groupby('label')['unique_word_count'].mean()

plt.figure(figsize=(8, 6))
diversity.plot(kind='bar', color=['red', 'green'])
plt.title('Diversidad Léxica Promedio por Clase (V o F)')
plt.xlabel('Clase')
plt.ylabel('Número Promedio de Palabras Únicas')
plt.xticks(ticks=[0, 1], labels=['Falsas', 'Verdaderas'], rotation=0)
plt.show()

"""***Análisis Estadístico y Visualización de Longitudes en Noticias Falsas y Verdaderas***

Este código realiza un análisis descriptivo de noticias falsas y verdaderas, calculando longitudes de títulos y textos, mostrando estadísticas por clase (falsas y verdaderas). El análisis se enfoca en comprender diferencias clave en la longitud de los textos.
"""

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Eliminar filas con valores nulos en 'title' y 'text'
dataset = dataset.dropna(subset=['title', 'text']).copy()

# Calcular la longitud de los títulos y textos usando .loc
dataset.loc[:, 'title_length'] = dataset['title'].apply(len)
dataset.loc[:, 'text_length'] = dataset['text'].apply(len)

# Resumen estadístico básico por clase
summary_statistics = dataset.groupby('label')[['title_length', 'text_length']].describe()
print("Resumen Estadístico Básico por Clase:")
print(summary_statistics)

# Promedios de longitud de títulos y textos por clase
average_lengths = dataset.groupby('label').agg(
    avg_title_length=('title_length', 'mean'),
    avg_text_length=('text_length', 'mean'),
    count=('title_length', 'size')
).reset_index()

# Longitud máxima y mínima por clase
max_lengths = dataset.groupby('label').agg(
    max_title_length=('title_length', 'max'),
    max_text_length=('text_length', 'max'),
    min_title_length=('title_length', 'min'),
    min_text_length=('text_length', 'min')
).reset_index()

# Unir estadísticas
summary = pd.merge(average_lengths, max_lengths, on='label')

print("\nResumen de Longitudes por Clase:")
print(summary)

# Visualización de promedios
plt.figure(figsize=(10, 6))
sns.barplot(x='label', y='avg_title_length', data=average_lengths, palette='pastel', hue='label', legend=False)
plt.title('Promedio de Longitud de Títulos por Etiqueta')
plt.xlabel('Etiqueta (0: Falsas, 1: Verdaderas)')
plt.ylabel('Promedio de Longitud del Título')
plt.xticks(ticks=[0, 1], labels=['Falsas', 'Verdaderas'])
plt.show()

plt.figure(figsize=(10, 6))
sns.barplot(x='label', y='avg_text_length', data=average_lengths, palette='pastel', hue='label', legend=False)
plt.title('Promedio de Longitud de Textos por Etiqueta')
plt.xlabel('Etiqueta (0: Falsas, 1: Verdaderas)')
plt.ylabel('Promedio de Longitud del Texto')
plt.xticks(ticks=[0, 1], labels=['Falsas', 'Verdaderas'])
plt.show()

# Histogramas de longitudes por clase
plt.figure(figsize=(12, 6))
sns.histplot(dataset[dataset['label'] == 0]['text_length'], bins=30, color='red', label='Falsas', kde=True)
sns.histplot(dataset[dataset['label'] == 1]['text_length'], bins=30, color='blue', label='Verdaderas', kde=True)
plt.title('Distribución de Longitudes de Textos por Clase')
plt.xlabel('Longitud del Texto')
plt.ylabel('Frecuencia')
plt.legend()
plt.show()

"""***Análisis de Métricas Textuales y Correlaciones***

Este código evalúa datos textuales mediante el conteo de valores nulos y el cálculo de métricas como la longitud promedio de oraciones, y la complejidad (carácteres por palabra). Se resumen estas métricas por clase y se analiza la correlación entre la longitud de textos y títulos con su veracidad, presentando los resultados en un mapa de calor.
"""

# Conteo de Valores Nulos
null_counts = dataset.isnull().sum()
print("Conteo de Valores Nulos por Columna:")
print(null_counts)

# Longitud promedio de oraciones
dataset['sentence_count'] = dataset['text'].apply(lambda x: len(x.split('.')) if x else 1)  # Usa 1 para evitar división por cero
dataset['sentence_length'] = dataset['text'].apply(lambda x: [len(sentence) for sentence in x.split('.')])  # Longitud de cada oración
dataset['average_sentence_length'] = dataset['sentence_length'].apply(lambda x: sum(x) / len(x) if len(x) > 0 else 0)  # Promedio de longitudes

# Diversidad léxica
dataset['unique_word_count'] = dataset['cleaned_text'].apply(lambda x: len(set(x.split())))

# Complejidad (promedio de caracteres por palabra)
dataset['complexity'] = dataset['cleaned_text'].apply(lambda x: sum(len(word) for word in x.split()) / len(x.split()) if x.split() else 0)

# Resumen de las nuevas métricas
text_analysis_summary = dataset[['label', 'unique_word_count', 'average_sentence_length', 'complexity']].groupby('label').mean()
print("\nResumen de Análisis de Texto por Clase:")
print(text_analysis_summary)

# Resumen de estadísticas descriptivas
stats_summary_text = dataset.groupby('label')['text_length'].describe()
stats_summary_title = dataset.groupby('label')['title_length'].describe()

# Calcular media, mediana, min, max y cuartiles
text_stats = dataset.groupby('label')['text_length'].agg(['mean', 'median', 'min', 'max', 'quantile'])
title_stats = dataset.groupby('label')['title_length'].agg(['mean', 'median', 'min', 'max', 'quantile'])

# Cuartiles específicos
text_stats['1st_quartile'] = dataset.groupby('label')['text_length'].quantile(0.25)
text_stats['3rd_quartile'] = dataset.groupby('label')['text_length'].quantile(0.75)

title_stats['1st_quartile'] = dataset.groupby('label')['title_length'].quantile(0.25)
title_stats['3rd_quartile'] = dataset.groupby('label')['title_length'].quantile(0.75)

# Imprimir resultados
print("Estadísticas para la longitud del texto:")
print(text_stats)
print("\nEstadísticas para la longitud del título:")
print(title_stats)

# Correlación entre la longitud del texto/título y su veracidad
correlation = dataset[['text_length', 'title_length', 'label']].corr()
print("\nMatriz de Correlación:")
print(correlation)

# Visualización de correlaciones
plt.figure(figsize=(8, 6))
sns.heatmap(correlation, annot=True, cmap='coolwarm', fmt='.2f')
plt.title('Correlación entre Longitud de Textos/Títulos y Veracidad')
plt.show()

import scipy.stats as stats

# ANOVA para comparar las longitudes del texto entre las clases
f_stat_text, p_value_text = stats.f_oneway(
    dataset[dataset['label'] == 0]['text_length'],  # Noticias falsas
    dataset[dataset['label'] == 1]['text_length']   # Noticias verdaderas
)

# ANOVA para comparar las longitudes del título entre las clases
f_stat_title, p_value_title = stats.f_oneway(
    dataset[dataset['label'] == 0]['title_length'],  # Noticias falsas
    dataset[dataset['label'] == 1]['title_length']   # Noticias verdaderas
)

# Imprimir
print(f"ANOVA Text Length F-statistic: {f_stat_text}, P-value: {p_value_text:.15f}")
print(f"ANOVA Title Length F-statistic: {f_stat_title}, P-value: {p_value_title:.15f}")

# Test t de Student para comparar las longitudes del texto entre las clases
t_stat_text, p_value_text_ttest = stats.ttest_ind(
    dataset[dataset['label'] == 0]['text_length'],  # Noticias falsas
    dataset[dataset['label'] == 1]['text_length'],  # Noticias verdaderas
)

# Test t de Student para comparar las longitudes del título entre las clases
t_stat_title, p_value_title_ttest = stats.ttest_ind(
    dataset[dataset['label'] == 0]['title_length'],  # Noticias falsas
    dataset[dataset['label'] == 1]['title_length'],  # Noticias verdaderas
)

# Imprimir
print(f"Test T Text Length T-statistic: {t_stat_text}, P-value: {p_value_text_ttest:.15f}")
print(f"Test T Title Length T-statistic: {t_stat_title}, P-value: {p_value_title_ttest:.15f}")

"""***Resumen Descriptivo de los Datos***

El código presentado permitió concluir que existen diferencias notables en las características y estadísticas descriptivas entre las noticias verdaderas y falsas. En particular, se observó que los títulos de las noticias verdaderas son significativamente más cortos, mientras que los textos de las noticias falsas son más largos en promedio. Esta disparidad sugiere que las noticias falsas tienden a ofrecer contenido más extenso, probablemente con el objetivo de desarrollar narrativas más complejas. Además, la mayor variabilidad en la longitud de los textos de las noticias verdaderas indica una diversidad en la extensión y estructura de su contenido. En términos de complejidad, aunque las noticias verdaderas tienen oraciones más largas, la diferencia en la complejidad global del contenido entre ambas clases es mínima. Los análisis estadísticos confirmaron que existen diferencias estadísticamente significativas en la longitud de los textos y títulos, especialmente en los títulos, lo que refuerza la idea de que los estilos de redacción varían entre las noticias verdaderas y falsas.

***Análisis Exploratorio de Datos (EDA)***

El código presentado permitió identificar patrones clave. En particular, los histogramas de la longitud de los títulos y textos originales frente a los limpiados demostraron cómo la limpieza de los datos mejora la concisión del contenido, facilitando así el análisis y la predicción. Además, la generación de nubes de palabras y el análisis de frecuencia revelaron que "said" (dicho) es una de las palabras más comunes en noticias falsas, sugiriendo que estas tienden a utilizar citas o declaraciones sin contexto. La palabra "Trump", aunque también frecuente en noticias verdaderas, reflejó el interés por figuras públicas, como políticos estadounidenses, aunque su uso y contexto pueden variar entre las categorías.

***Desde el análisis...***

Las palabras más comunes desde el atributo de contenido de las noticias, tanto para noticias falsas como verdaderas, revela que ciertos términos tienden a aparecer con mayor frecuencia en cada tipo de noticia, lo cual puede ayudar a diferenciar entre información veraz y falsa en modelos de detección de fake news. En las noticias falsas, las palabras predominantes son "said", "trump", "mr", "state", "would", "u", "president", "year", "one" y "new". En contraste, las noticias verdaderas suelen utilizar con mayor frecuencia palabras como "trump", "u", "people", "said", "clinton", "one", "would", "state", "president" y "time". Estas palabras, además de ser útiles para la clasificación, sugieren que el contenido de las noticias, y por tanto el enfoque de las mismas, está en gran medida relacionado con el ámbito político, en específico, con las elecciones presidenciales de Estados Unidos de hace dos ciclos.

***Métodos, Técnicas de visualización y Técnicas para analizar la Importancia de Características***

En este proyecto, la clase de hipótesis estará compuesta por un conjunto de modelos que el algoritmo Random Forest puede generar, considerando las diversas combinaciones de las características textuales del título y contenido de las noticias, las cuales se utilizarán para clasificar las noticias como verdaderas o falsas.

La investigación se enfocará en el concepto de la función de pérdida. En particular, se utilizará la Entropía Cruzada Binaria, una de las funciones de pérdida más comunes en problemas de clasificación binaria, como en el caso de clasificar noticias como verdaderas o falsas. Esta función mide la discrepancia entre las probabilidades predichas por el modelo y las etiquetas verdaderas, evaluando la efectividad del modelo para predecir la clase correcta.

Para evaluar el rendimiento del modelo en la clasificación de noticias falsas, se utilizarán varias métricas clave: precisión (que indica la proporción de verdaderos positivos entre las predicciones positivas), recall (que mide la capacidad del modelo para identificar correctamente los casos positivos reales) y F1 score (que combina precisión y recall, proporcionando un balance entre ambas). Además, se analizarán la matriz de confusión, que ofrece una visión detallada de los aciertos y errores del modelo, y el AUC-ROC (Área Bajo la Curva ROC), que mide la capacidad del modelo para distinguir entre las clases a diferentes umbrales de probabilidad. Un AUC cercano a 1 indica un buen desempeño, mientras que uno cercano a 0.5 sugiere que el modelo tiene un rendimiento cercano al azar.

Para complementar estas métricas, se incorporarán técnicas de visualización y análisis de la importancia de las características. La Curva ROC y el AUC-ROC permitirán evaluar la capacidad del modelo para diferenciar entre noticias verdaderas y falsas, mientras que la matriz de confusión proporcionará una visión clara de los aciertos y errores cometidos. También se utilizarán gráficos de barras y boxplots para analizar las distribuciones de variables cuantitativas entre las distintas clases.

En cuanto a la interpretación de los resultados, se emplearán enfoques como la Importancia de Características en Random Forest, que analiza qué atributos del modelo contribuyen más a su precisión. Asimismo, se utilizarán métodos como LIME y SHAP para interpretar el impacto de las características individuales en las decisiones del modelo, proporcionando mayor transparencia y comprensión de las predicciones. Por último, las técnicas de permutación se emplearán para medir la relevancia de las características observando cómo varía el rendimiento del modelo cuando se altera el orden de las mismas.

***Sobre Random Forest...***

Random Forest es un método de clasificación que combina múltiples árboles de decisión para predecir la veracidad de las noticias. Cada árbol se entrena con diferentes subconjuntos de datos, capturando patrones complejos y no lineales en el contenido textual. A través de un proceso de promediado, el modelo genera una predicción más robusta y confiable sobre si una noticia es falsa o verdadera. Esta técnica es especialmente efectiva para manejar la diversidad de características

Elegimos Random Forest directamente en lugar de comenzar con un Árbol de Decisión (DT) debido a la complejidad y variabilidad de los datos en la clasificación de noticias falsas y verdaderas. RF, al combinar múltiples árboles entrenados con subconjuntos aleatorios de los datos, proporciona una mayor robustez y reduce el riesgo de sobreajuste, lo que resulta en un modelo más preciso y confiable desde el principio.

***Análisis y Modelado de Datos con Random Forest: Evaluación, Importancia de Características y Explicación de Predicciones***

Este código realiza un análisis y modelado de datos utilizando un clasificador Random Forest. Primero, carga y preprocesa los datos de entrenamiento y prueba, asegurándose de que las matrices de características sean adecuadas para el modelado. Luego, selecciona muestras aleatorias de los conjuntos de entrenamiento y prueba para ser utilizadas en la evaluación del modelo. A continuación, entrena el modelo Random Forest con los datos de entrenamiento y evalúa su desempeño utilizando varias métricas, como la precisión, el reporte de clasificación y la matriz de confusión. También se visualiza la importancia de las características en el modelo, la curva ROC con su AUC, y se realiza un análisis de importancia de características mediante permutación. Finalmente, el código incluye una explicación de las predicciones utilizando LIME y el cálculo de valores SHAP para interpretar el modelo.
"""

# Importar bibliotecas necesarias para la carga de datos y preprocesamiento
import joblib
import numpy as np

# Cargar los datos preprocesados y el vectorizador TF-IDF
X_train, X_test, y_train, y_test = joblib.load('preprocessed_data.pkl')
vectorizer = joblib.load('tfidf_vectorizer.pkl')

# Convertir matrices dispersas a NumPy arrays si es necesario
X_train = X_train.toarray() if not isinstance(X_train, np.ndarray) else X_train
X_test = X_test.toarray() if not isinstance(X_test, np.ndarray) else X_test
y_train = np.array(y_train)
y_test = np.array(y_test)

# Definir y seleccionar muestras de entrenamiento y prueba
sample_size_train, sample_size_test = min(1000, X_train.shape[0]), min(1000, X_test.shape[0])
train_indices = np.random.choice(X_train.shape[0], sample_size_train, replace=False)
test_indices = np.random.choice(X_test.shape[0], sample_size_test, replace=False)
X_train_sample, y_train_sample = X_train[train_indices], y_train[train_indices]
X_test_sample, y_test_sample = X_test[test_indices], y_test[test_indices]

# Importar bibliotecas para el modelado y evaluación
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, roc_curve, roc_auc_score
from sklearn.inspection import permutation_importance

# Implementación y evaluación del modelo Random Forest
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train_sample, y_train_sample)
y_pred_rf = rf_model.predict(X_test_sample)

print("Reporte de Clasificación para Random Forest:")
print(classification_report(y_test_sample, y_pred_rf))
print(f"Precisión del modelo: {accuracy_score(y_test_sample, y_pred_rf):.2f}")

# Generar y mostrar la matriz de confusión
conf_matrix = confusion_matrix(y_test_sample, y_pred_rf)
sns.heatmap(conf_matrix / np.sum(conf_matrix), annot=True, fmt=".2%", cmap="Blues")
plt.xlabel("Predicción")
plt.ylabel("Real")
plt.title("Matriz de Confusión Normalizada")
plt.show()

# Visualizar importancia de características en el modelo Random Forest
importances = rf_model.feature_importances_
feature_names = vectorizer.get_feature_names_out()
feature_importance = pd.DataFrame({'Feature': feature_names, 'Importance': importances}).sort_values(by='Importance', ascending=False)

plt.figure(figsize=(12, 8))
plt.barh(feature_importance['Feature'][:20], feature_importance['Importance'][:20], color='skyblue')
plt.xlabel('Importancia')
plt.title('20 Características Más Importantes en el Modelo Random Forest')
plt.gca().invert_yaxis()
plt.show()

# Curva ROC y AUC para el modelo Random Forest
y_proba_rf = rf_model.predict_proba(X_test_sample)[:, 1]
fpr, tpr, _ = roc_curve(y_test_sample, y_proba_rf)
roc_auc = roc_auc_score(y_test_sample, y_proba_rf)
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='blue', label=f'AUC = {roc_auc:.2f}')
plt.plot([0, 1], [0, 1], color='gray', linestyle='--')
plt.xlabel('Tasa de Falsos Positivos')
plt.ylabel('Tasa de Verdaderos Positivos')
plt.title('Curva ROC para Random Forest')
plt.legend(loc="lower right")
plt.show()

# Análisis de importancia de características por permutación
perm_importance = permutation_importance(rf_model, X_test_sample, y_test_sample, n_repeats=5, random_state=42)
sorted_idx = perm_importance.importances_mean.argsort()[-20:]

plt.figure(figsize=(12, 8))
plt.barh(range(20), perm_importance.importances_mean[sorted_idx], color="skyblue")
plt.yticks(range(20), np.array(feature_names)[sorted_idx])
plt.xlabel("Importancia de la Característica")
plt.title("Importancia de Características por Permutación en Random Forest")
plt.show()

# Explicación de predicciones con LIME
!pip install lime
from lime.lime_text import LimeTextExplainer
explainer = LimeTextExplainer(class_names=['Verdadero', 'Falso'])
text_instance_str = " ".join(vectorizer.inverse_transform(X_test_sample[0:1])[0])

explanation = explainer.explain_instance(
    text_instance_str,
    lambda x: rf_model.predict_proba(vectorizer.transform(x).toarray()),
    num_features=10
)
explanation.show_in_notebook()

# Importar SHAP
import shap
# Cálculo de valores SHAP para el modelo de Random Forest
explainer_shap = shap.TreeExplainer(rf_model)
shap_values = explainer_shap.shap_values(X_test_sample.astype(float), check_additivity=False)
print("Valores SHAP calculados con éxito.")

"""***Validación de supuesto(s) RF***

El modelo de Random Forest, al no depender de supuestos estrictos sobre la distribución de los datos ni sobre la linealidad, demostró ser flexible y eficaz en el análisis de datos complejos como los de noticias. La validación del modelo se llevó a cabo mediante un análisis de la importancia de las características, en el cual se visualizó qué palabras clave, como "reuters", "said" y "monday", tenían mayor influencia en las decisiones del modelo.

***Análisis RF***

El modelo Random Forest demuestra un rendimiento sobresaliente en la clasificación de noticias verdaderas y falsas. Es particularmente eficaz en identificar noticias verdaderas, minimizando los falsos negativos, y es capaz de capturar relaciones complejas en los datos. Este modelo tiene una ventaja en cuanto a su flexibilidad para manejar interacciones entre diferentes características, lo que lo hace más robusto para tareas de clasificación complicadas. Su interpretación mediante la herramienta Lime indica que se apoya en términos específicos de noticias, como nombres de fuentes confiables, para hacer predicciones acertadas sobre la veracidad de las noticias.

***Sobre la Regresión Logística...***

Para el caso de estudio, nos pareció relevante probar con la Regresión Logística dado que es posible emplearla como un método de clasificación binaria que predice la probabilidad de que una noticia pertenezca a la categoría de falsa o verdadera. Además, permite establecer una relación probabilística entre las palabras del título y contenido y la probabilidad de veracidad. Esto es especialmente útil en el contexto de clasificación de noticias.

***Análisis y Modelado de Datos con Regresión Logística: Evaluación, Reducción de Dimensionalidad, Importancia de Características y Explicación de Predicciones***

Este código realiza un análisis y modelado de datos utilizando diversas técnicas de procesamiento, reducción de dimensionalidad, selección de características y evaluación de modelos. Primero, carga los datos preprocesados y el vectorizador TF-IDF, luego selecciona una muestra aleatoria para trabajar con ellos. Se aplica escalado de características y reducción de dimensionalidad mediante PCA. Después, se ajusta un modelo de regresión logística sobre las características seleccionadas y se evalúa utilizando métricas como la precisión, la matriz de confusión, y la curva ROC-AUC. También se analizan la importancia de las características mediante permutación y se visualizan las características más influyentes. El código incluye explicaciones de las predicciones del modelo mediante LIME y SHAP para ofrecer interpretaciones más comprensibles sobre el comportamiento del modelo, y muestra gráficamente la importancia de cada característica.
"""

# Importar bibliotecas necesarias
import joblib
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, roc_curve, roc_auc_score
from sklearn.inspection import permutation_importance
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.feature_selection import SelectKBest, f_classif
from statsmodels.stats.outliers_influence import variance_inflation_factor
from lime.lime_text import LimeTextExplainer
import shap

# Cargar datos preprocesados y el vectorizador TF-IDF
X_train, X_test, y_train, y_test = joblib.load('preprocessed_data.pkl')
vectorizer = joblib.load('tfidf_vectorizer.pkl')

# Convertir matrices dispersas a arrays de NumPy si es necesario
X_train = X_train.toarray() if not isinstance(X_train, np.ndarray) else X_train
X_test = X_test.toarray() if not isinstance(X_test, np.ndarray) else X_test
y_train = np.array(y_train)
y_test = np.array(y_test)

# Muestra aleatoria para análisis rápido
sample_size_train, sample_size_test = min(1000, X_train.shape[0]), min(1000, X_test.shape[0])
train_indices = np.random.choice(X_train.shape[0], sample_size_train, replace=False)
test_indices = np.random.choice(X_test.shape[0], sample_size_test, replace=False)
X_train_sample, y_train_sample = X_train[train_indices], y_train[train_indices]
X_test_sample, y_test_sample = X_test[test_indices], y_test[test_indices]

# Escalado de datos y reducción de dimensionalidad con PCA
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train_sample)
X_test_scaled = scaler.transform(X_test_sample)

# Seleccionar mejores características y calcular VIF solo en el subconjunto de características
selector = SelectKBest(score_func=f_classif, k=100)
X_train_selected = selector.fit_transform(X_train_scaled, y_train_sample)
X_test_selected = selector.transform(X_test_scaled)

# Cálculo del VIF en características seleccionadas
vif_data = pd.DataFrame({
    "feature": [f"Feature_{i}" for i in range(X_train_selected.shape[1])],
    "VIF": [variance_inflation_factor(X_train_selected, i) for i in range(X_train_selected.shape[1])]
})
print("Características con alta colinealidad (VIF > 10):")
print(vif_data[vif_data["VIF"] > 10])

# Aplicar PCA
pca = PCA(n_components=0.95)
X_train_pca = pca.fit_transform(X_train_selected)
X_test_pca = pca.transform(X_test_selected)

import statsmodels.api as sm  # Importar statsmodels

# Ajustar un modelo de regresión logística usando statsmodels
X_train_pca_with_const = sm.add_constant(X_train_pca)  # Agregar constante
logreg_model_sm = sm.Logit(y_train_sample, X_train_pca_with_const).fit()  # Ajustar modelo
print(logreg_model_sm.summary())  # Mostrar resumen del modelo

# Obtener la matriz de cargas del PCA
loadings = pca.components_

# Asegurarse de que el número de características coincida
if loadings.shape[1] != X_train_selected.shape[1]:
    print("Error: El número de componentes del PCA no coincide con el número de características seleccionadas.")
else:
    # Convertir las cargas a un DataFrame
    loading_matrix = pd.DataFrame(loadings.T, index=[f"Feature_{i}" for i in range(X_train_selected.shape[1])], columns=[f"PC{i+1}" for i in range(loadings.shape[0])])

    # Mostrar las 10 características más influyentes para cada componente principal
    for i in range(min(5, loading_matrix.shape[1])):  # Cambia 5 al número de componentes que deseas analizar
        print(f"\nCaracterísticas más influyentes en PC{i+1}:")
        print(loading_matrix[f"PC{i+1}"].sort_values(ascending=False).head(10))

# Implementación y evaluación del modelo de Regresión Logística
logreg_model = LogisticRegression(max_iter=1000, random_state=42)
logreg_model.fit(X_train_pca, y_train_sample)
y_pred_logreg = logreg_model.predict(X_test_pca)

# Reporte de clasificación y precisión del modelo
print("Reporte de Clasificación para Regresión Logística:")
print(classification_report(y_test_sample, y_pred_logreg))
print(f"Precisión del modelo: {accuracy_score(y_test_sample, y_pred_logreg):.2f}")

# Matriz de confusión
conf_matrix_logreg = confusion_matrix(y_test_sample, y_pred_logreg)
sns.heatmap(conf_matrix_logreg / np.sum(conf_matrix_logreg), annot=True, fmt=".2%", cmap="Blues")
plt.xlabel("Predicción")
plt.ylabel("Real")
plt.title("Matriz de Confusión Normalizada - Regresión Logística")
plt.show()

# Curva ROC y AUC
y_proba_logreg = logreg_model.predict_proba(X_test_pca)[:, 1]
fpr_logreg, tpr_logreg, _ = roc_curve(y_test_sample, y_proba_logreg)
roc_auc_logreg = roc_auc_score(y_test_sample, y_proba_logreg)
plt.figure(figsize=(8, 6))
plt.plot(fpr_logreg, tpr_logreg, color='blue', label=f'AUC = {roc_auc_logreg:.2f}')
plt.plot([0, 1], [0, 1], color='gray', linestyle='--')
plt.xlabel('Tasa de Falsos Positivos')
plt.ylabel('Tasa de Verdaderos Positivos')
plt.title('Curva ROC para Regresión Logística')
plt.legend(loc="lower right")
plt.show()

# Análisis de importancia de características con permutación
perm_importance_logreg = permutation_importance(logreg_model, X_test_pca, y_test_sample, n_repeats=5, random_state=42)
sorted_idx_logreg = perm_importance_logreg.importances_mean.argsort()[-20:]

plt.figure(figsize=(12, 8))
plt.barh(range(20), perm_importance_logreg.importances_mean[sorted_idx_logreg], color="skyblue")
plt.yticks(range(20), [f"PC{i+1}" for i in sorted_idx_logreg])
plt.xlabel("Importancia de la Característica")
plt.title("Importancia de Características por Permutación en Regresión Logística")
plt.show()

# Seleccionar las cargas de PC1 y ordenar
pc1_loadings = loading_matrix["PC1"].sort_values(ascending=False).head(10)

# Graficar las características más influyentes en PC1
plt.figure(figsize=(10, 6))
pc1_loadings.plot(kind="bar", color="skyblue")
plt.title("Características más influyentes en PC1")
plt.xlabel("Características")
plt.ylabel("Carga")
plt.show()

# Explicación de predicciones con LIME
explainer_logreg = LimeTextExplainer(class_names=['Verdadero', 'Falso'])

# Elige un texto de prueba (puedes cambiar el índice para probar con diferentes ejemplos)
text_instance_str_logreg = " ".join(vectorizer.inverse_transform(X_test_sample[0:1])[0])

# Función para predecir probabilidades adecuadamente
def predict_proba(texts):
    # Convertir texto a características usando el vectorizador
    tfidf_features = vectorizer.transform(texts).toarray()
    # Escalar las características
    scaled_features = scaler.transform(tfidf_features)
    # Seleccionar las mejores características
    selected_features = selector.transform(scaled_features)
    # Aplicar PCA
    pca_features = pca.transform(selected_features)
    # Devolver las probabilidades
    return logreg_model.predict_proba(pca_features)

# Explicación del texto
explanation_logreg = explainer_logreg.explain_instance(
    text_instance_str_logreg,
    predict_proba,
    num_features=10
)

# Mostrar la explicación en el cuaderno
explanation_logreg.show_in_notebook()


# Cálculo de valores SHAP
explainer_shap_logreg = shap.Explainer(logreg_model, X_train_pca)
shap_values_logreg = explainer_shap_logreg(X_test_pca)
print("Valores SHAP calculados con éxito para la Regresión Logística.")

# Gráfico de Resumen de valores SHAP
shap.summary_plot(shap_values_logreg, X_test_pca)

# Gráfico de dependencia para una característica específica
# Aquí, elige un índice de característica que tenga sentido
feature_index = 0  # Índice de la característica que deseas graficar
shap.dependence_plot(feature_index, shap_values_logreg.values, X_test_pca)

plt.show()

"""***Validación de supuesto(s) RL***

En el análisis realizado, se validaron los dos supuestos clave de la regresión logística. Primero, se evaluó la multicolinealidad mediante el cálculo del Factor de Inflación de la Varianza (VIF), y los resultados indicaron que no existía una multicolinealidad significativa entre las variables independientes. Esto confirmó que el modelo no presentaba problemas de multicolinealidad y que los coeficientes obtenidos eran interpretables.

En segundo lugar, se verificó el supuesto de que un cambio unitario en las variables independientes se relaciona linealmente con el logaritmo de las probabilidades de la variable dependiente. Para ello, se ajustó un modelo de regresión logística, y se observó que el modelo se ajustaba bien a los datos, lo que sugirió que el supuesto de linealidad también se cumplía adecuadamente.

Se decidieron incluir todas las variables en el modelo de regresión logística, incluso aquellas con un p-valor mayor a 0.05, para evitar el sesgo de omisión de posibles factores relevantes. Aunque algunas variables no son significativas individualmente, su inclusión se basa en la posibilidad de que interactúen con otras o tengan un efecto combinado que no se refleja en un análisis aislado. Además, el modelo completo permite una visión más exhaustiva de las relaciones entre las variables, y el buen ajuste general (Pseudo R-squared de 0.7718) justifica mantener todas las variables.

***Análisis RL***

El modelo de Regresión Logística también muestra un buen desempeño, pero se enfoca más en evitar falsos positivos, lo que puede llevar a que algunas noticias verdaderas sean clasificadas incorrectamente como falsas. Aunque es efectivo en su tarea, la Regresión Logística tiene limitaciones al manejar relaciones no lineales entre las variables, lo que puede afectar su capacidad para capturar interacciones complejas en los datos. Además, su interpretación a través de SHAP resalta la importancia de ciertas características, pero con un enfoque más lineal que el de Random Forest.
"""

import matplotlib.pyplot as plt
from sklearn.metrics import recall_score
from statsmodels.stats.outliers_influence import variance_inflation_factor

# Análisis gráfico de residuos para verificar supuestos del modelo
try:
    print("Analizando residuos...")

    # Para Random Forest
    predicted_proba_rf = rf_model.predict_proba(X_train_sample)[:, 1]  # Probabilidades predichas
    residuals_rf = y_train_sample - predicted_proba_rf

    plt.figure(figsize=(12, 6))

    # Gráfico de residuos para Random Forest
    plt.subplot(1, 2, 1)
    plt.scatter(predicted_proba_rf, residuals_rf)
    plt.xlabel("Probabilidad Predicha (Random Forest)")
    plt.ylabel("Residuos")
    plt.title("Gráfico de Residuos - Random Forest")
    plt.axhline(0, color='red', linestyle='--')

    # Para Regresión Logística
    predicted_proba_logreg = logreg_model.predict_proba(X_train_pca)[:, 1]  # Probabilidades predichas
    residuals_logreg = y_train_sample - predicted_proba_logreg

    # Gráfico de residuos para Regresión Logística
    plt.subplot(1, 2, 2)
    plt.scatter(predicted_proba_logreg, residuals_logreg)
    plt.xlabel("Probabilidad Predicha (Regresión Logística)")
    plt.ylabel("Residuos")
    plt.title("Gráfico de Residuos - Regresión Logística")
    plt.axhline(0, color='red', linestyle='--')

    plt.tight_layout()
    plt.show()

except Exception as e:
    print("Error analizando residuos:", e)

# Evaluación de modelos usando Recall
try:
    print("Evaluando modelos usando Recall...")

    # Predicciones de Random Forest
    y_pred_tree = rf_model.predict(X_test_sample)
    recall_tree = recall_score(y_test_sample, y_pred_tree)
    print(f"Recall del Random Forest: {recall_tree:.2f}")

    # Predicciones de Regresión Logística
    y_pred_logistic = logreg_model.predict(X_test_pca)
    recall_logistic = recall_score(y_test_sample, y_pred_logistic)
    print(f"Recall de la Regresión Logística: {recall_logistic:.2f}")

    # Verificar si los modelos cumplen con el objetivo de recall >= 0.85
    if recall_tree >= 0.85:
        print("El modelo de Random Forest cumple con el objetivo de recall >= 0.85")
    else:
        print("El modelo de Random Forest no cumple con el objetivo de recall >= 0.85")

    if recall_logistic >= 0.85:
        print("El modelo de Regresión Logística cumple con el objetivo de recall >= 0.85")
    else:
        print("El modelo de Regresión Logística no cumple con el objetivo de recall >= 0.85")

except Exception as e:
    print("Error evaluando modelos usando Recall:", e)

"""***Análisis Comparativo: RF v/s RL***

Ambos modelos son útiles para la clasificación de noticias, pero Random Forest es más adecuado para este caso específico debido a su mayor capacidad para manejar interacciones complejas y su efectividad en la detección de noticias verdaderas, lo que lo convierte en la opción más sólida para el análisis de veracidad en noticias.
"""

import joblib

# Guardar el modelo entrenado en un archivo .pkl
joblib.dump(rf_model, "rf_model.pkl")

# Guardar el vectorizador también (si no se ha guardado)
joblib.dump(vectorizer, "tfidf_vectorizer.pkl")
print("Modelo y vectorizador guardados correctamente.")

"""***¿Recomendaciones?***

En el contexto de nuestro estudio, no tiene mucho sentido generar recomendaciones más allá de la indicación de veracidad o falsedad que el modelo RF proporciona. El objetivo principal del modelo es clasificar las noticias como verídicas o falsas, y cualquier recomendación adicional podría estar fuera del alcance del modelo y del análisis actual. Por lo tanto, las indicaciones de veracidad del modelo son suficientes para el caso en estudio.

***Detector de Noticias Falsas con Interfaz Gradio: Interfaz, Explicablidad y Feedback***

Este código implementa una aplicación interactiva para detectar si una noticia es "falsa" o "verdadera" utilizando un modelo de aprendizaje automático, específicamente un modelo de Random Forest. Primero, carga dos archivos previamente entrenados con joblib: uno es un vectorizador TF-IDF, que transforma el texto de entrada en un formato adecuado para el modelo, y el otro es el propio modelo de Random Forest, que clasifica el texto en las categorías de "falsa" o "verdadera". Para hacer el modelo más interpretable, se utiliza LIME (Local Interpretable Model-Agnostic Explanations), herramienta que explica cómo se toman las decisiones del modelo, mostrando qué características del texto (como las palabras) influyen en la predicción.

El código contiene varias funciones; una para realizar la predicción de la veracidad de la noticia, otra para obtener las probabilidades de cada clase, y una más para generar una explicación visual de la predicción utilizando LIME. Además, si el usuario lo solicita, la explicación se presenta visualmente mediante un gráfico.

También se guarda el feedback del usuario sobre la precisión de la predicción en un archivo CSV.

La interfaz de usuario, construida con Gradio, permite que el usuario ingrese el texto de una noticia, vea la predicción de su veracidad y, si lo desea, la explicación de LIME. Los resultados incluyen la predicción de la noticia, la probabilidad asociada y la imagen con la explicación del modelo.
"""

import joblib
import numpy as np
import gradio as gr
import lime
from lime.lime_text import LimeTextExplainer
import matplotlib.pyplot as plt
import tempfile
import pandas as pd

# Cargar el modelo y el vectorizador TF-IDF
vectorizer = joblib.load("tfidf_vectorizer.pkl")
rf_model = joblib.load("rf_model.pkl")

# Crear un explainer de LIME
explainer = LimeTextExplainer(class_names=['Falsa', 'Verdadera'])

# Ruta para guardar el feedback
feedback_file = 'feedback.csv'

# Función para predecir si una noticia es falsa o verdadera
def predict_news(text):
    try:
        # Transformar el texto utilizando el vectorizador cargado
        text_transformed = vectorizer.transform([text])

        # Obtener la predicción del modelo
        prediction = rf_model.predict(text_transformed)[0]
        prob = rf_model.predict_proba(text_transformed)[0][int(prediction)]

        # Asignar la etiqueta según la predicción
        label = 'Falsa' if prediction == 0 else 'Verdadera'

        return label, prob
    except Exception as e:
        return f"Error: {str(e)}", 0.0

# Función para predecir probabilidades usando el modelo Random Forest (para LIME)
def predict_probabilities(texts):
    transformed_texts = vectorizer.transform(texts).toarray()  # TF-IDF Transformation
    return rf_model.predict_proba(transformed_texts)  # Devolver probabilidades para ambas clases

# Función para explicar una predicción usando LIME y devolver la imagen de la explicación
def explain_prediction_with_lime(text):
    # Predecir la clase de la noticia
    prediction_probabilities = predict_probabilities([text])
    predicted_class = np.argmax(prediction_probabilities)  # Clase con la probabilidad más alta
    predicted_label = 'Falsa' if predicted_class == 0 else 'Verdadera'

    # Explicación usando LIME
    explanation = explainer.explain_instance(
        text,
        predict_probabilities,  # Usamos el modelo para obtener probabilidades
        num_features=10  # Número de características a mostrar en la explicación
    )

    # Mostrar la explicación usando Matplotlib
    fig = explanation.as_pyplot_figure()

    # Eliminar el título por defecto de LIME
    fig.axes[0].set_title('')  # Eliminar el título por defecto

    # Cambiar el título del gráfico para reflejar la predicción real
    fig.suptitle(f"Local Explanation for {predicted_label} News", fontsize=14)

    # Guardar la figura en un archivo temporal
    with tempfile.NamedTemporaryFile(delete=False, suffix='.png') as tmpfile:
        fig.savefig(tmpfile.name)  # Guardar la explicación como imagen
        tmpfile.close()  # Cerrar el archivo para asegurarnos de que se guarde

    return tmpfile.name  # Devolver la ruta del archivo de imagen

# Función combinada que retorna la predicción y, si se solicita, la explicación de LIME
def combined_function(text, show_explanation=False, user_feedback=""):
    # Predicción de la noticia
    label, prob = predict_news(text)

    if show_explanation:
        # Explicación con LIME
        explanation_image = explain_prediction_with_lime(text)
    else:
        explanation_image = None

    # Guardar el feedback del usuario si se proporciona
    if user_feedback:
        feedback_data = pd.DataFrame([[text, label, prob, user_feedback]],
                                     columns=['Texto', 'Predicción', 'Probabilidad', 'Feedback Usuario'])
        feedback_data.to_csv(feedback_file, mode='a', header=False, index=False)

    return label, prob, explanation_image

# Crear la interfaz de usuario con Gradio
iface = gr.Interface(
    fn=combined_function,
    inputs=[gr.Textbox(lines=5, placeholder="Introduce el texto de la noticia aquí..."),
            gr.Checkbox(label="Mostrar explicación de LIME"),
            gr.Textbox(lines=2, placeholder="Proporciona tu feedback (opcional)...")],  # Feedback de usuario
    outputs=[gr.Text(label="Predicción"), gr.Number(label="Probabilidad"), gr.Image(type="filepath")],
    title="Detector de Noticias Falsas con Feedback",
    description="Escribe una noticia, obtén una predicción sobre su veracidad y, si lo deseas, una explicación interactiva con LIME. Proporciona tu feedback sobre la precisión de la predicción."
)

# Lanzar la interfaz
iface.launch()

"""***Guardar Feedback en Google Drive***

Este código monta Google Drive en un entorno de Google Colab para permitir el acceso a archivos en el almacenamiento de Drive. Luego, copia el archivo feedback.csv desde el entorno de Colab a una carpeta específica en Google Drive (/content/drive/MyDrive/), lo que facilita guardar y acceder a los datos de retroalimentación proporcionados por el usuario.
"""

from google.colab import drive
drive.mount('/content/drive')

# Guardar el feedback en Google Drive
!cp /content/feedback.csv /content/drive/MyDrive/

"""***Referencias***

1. Awan, A. A. (2024, marzo 13). Una introducción a los valores SHAP y a la interpretabilidad del machine learning. Datacamp.com. https://www.datacamp.com/es/tutorial/introduction-to-shap-values-machine-learning-interpretability
2. Blanco, P. (2024, abril 17). Los algoritmos random forest, una técnica muy potente de machine learning. Educaopen.com. https://www.educaopen.com/digital-lab/blog/inteligencia-artificial/algoritmo-random-forest
3. Clasificación: ROC y AUC. (s/f). Google for Developers. Recuperado el 31 de octubre de 2024, de https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc?hl=es-419
4. Cordis, C. (2021, 27 agosto). Un verificador de datos impulsado por inteligencia artificial ayuda a la lucha contra las noticias falsas en los medios de comunicación. CORDIS | European Commission. https://cordis.europa.eu/article/id/430588-ai-factcheckers-aid-the-battle-against-fake-news/es
5. Comprensión de la importancia de la permutación. (s/f). Qlik.com. Recuperado el 31 de octubre de 2024, de https://help.qlik.com/es-ES/cloud-services/Subsystems/Hub/Content/Sense_Hub/AutoML/permutation-importance.htm
6. Daniel. (2022, enero 25). Random Forest: Bosque aleatorio. Definición y funcionamiento. Formación en ciencia de datos | Datascientest.com; DataScientest. https://datascientest.com/es/random-forest-bosque-aleatorio-definicion-y-funcionamiento
7. DataCamp. (2024, mayo 3). Explainable AI, LIME & SHAP for model interpretability. From https://www.datacamp.com/es/tutorial/explainable-ai-understanding-and-trusting-machine-learning-models
8. Díaz, R. (2020, mayo 8). Métricas de Clasificación. The Machine Learners. https://www.themachinelearners.com/metricas-de-clasificacion/
9. Fake News: el peligro de las noticias falsas y su impacto en la ciudadanía. (2022, julio 28). Uchile.cl. https://uchile.cl/noticias/188632/fake-news-el-peligro-de-la-desinformacion-y-su-impacto
10. Huet, P. (2024, julio 4). Técnicas clave para el procesamiento de texto en NLP. Openwebinars.net. https://openwebinars.net/blog/tecnicas-clave-para-procesamiento-texto-nlp/
11. Métricas clave de los modelos de clasificación. (2024, abril 2). Codelabsacademy.com. https://codelabsacademy.com/es/blog/key-metrics-for-classification-models
12. Métricas De Evaluación De Modelos En El Aprendizaje Automático. (2023, septiembre 25). DataSource.ai. https://www.datasource.ai/es/data-science-articles/metricas-de-evaluacion-de-modelos-en-el-aprendizaje-automatico
13. Mesones, E. G. (2023, septiembre 13). Explicabilidad de modelos de ML: LIME - LatinXinAI - Medium. LatinXinAI. https://medium.com/latinxinai/explicabilidad-de-modelos-de-ml-lime-f9d0dceb5154
14. Porto, J. P. (2022, junio 22). Fake news. Definición.de; Definicion.de. https://definicion.de/fake-news/
15. Precision and recall at K in ranking and recommendations. (s/f). Evidentlyai.com. Recuperado el 31 de octubre de 2024, de https://www.evidentlyai.com/ranking-metrics/precision-recall-at-k
16. RPubs - Evaluación de supuestos en regresión logística. (s/f). Rpubs.com. Recuperado el 8 de noviembre de 2024, de https://rpubs.com/StephanyL/supuestos_regresion_logistica
17. Shahane, S. (2023). Fake News Classification [Data set].
18. Sarwade, S. (2023, septiembre 5). PNL simplificada - Parte 1 - Limpieza y preprocesamiento de textos. Geekflare Spain. https://geekflare.com/es/nlp-text-cleaning-and-preprocessing/
19. SPSS statistics subscription - classic. (2024, septiembre 30). Ibm.com. https://www.ibm.com/docs/es/spss-statistics/saas?topic=regression-logistic
20. Torres, B. (2020, junio 1). ¿Qué son las Fake News? UNAM Global - De la comunidad para la comunidad; UNAM Global. https://unamglobal.unam.mx/global_revista/que-son-fake-news/
21. Todo lo que necesita saber sobre el preprocesamiento de texto para NLP y Machine Learning. (2020, diciembre 23). ICHI.PRO. https://ichi.pro/es/todo-lo-que-necesita-saber-sobre-el-preprocesamiento-de-texto-para-nlp-y-machine-learning-206829911408487
22. Una guía rápida para la función de pérdida de entropía cruzada. (2021, junio 8). ICHI.PRO. https://ichi.pro/es/una-guia-rapida-para-la-funcion-de-perdida-de-entropia-cruzada-157453785000625
23. ¿Qué es Random Forest? (2023, mayo 3). Ibm.com. https://www.ibm.com/mx-es/topics/random-forest
24. ¿Qué es una matriz de confusión? (2024, octubre 15). Ibm.com. https://www.ibm.com/es-es/topics/confusion-matrix
25. ¿Qué es el aprendizaje supervisado? (2024, mayo 10). Ibm.com. https://www.ibm.com/es-es/topics/supervised-learning
26. ¿Qué es la visualización de datos? (2023, noviembre 7). Ibm.com. https://www.ibm.com/mx-es/topics/data-visualization
"""